--
-- Queries that lead to hanging (not dead lock) when we don't handle synchronization properly in shared scan
-- Queries that lead to wrong result when we don't finish executing the subtree below the shared scan being squelched.
--
-- start_ignore
CREATE EXTENSION IF NOT EXISTS gp_inject_fault;
-- end_ignore
CREATE SCHEMA shared_scan;
SET search_path = shared_scan;
CREATE TABLE foo (a int, b int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE TABLE bar (c int, d int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE TABLE jazz(e int, f int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'e' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO foo values (1, 2);
INSERT INTO bar SELECT i, i from generate_series(1, 100)i;
INSERT INTO jazz VALUES (2, 2), (3, 3);
ANALYZE foo;
ANALYZE bar;
ANALYZE jazz;
SELECT $query$
SELECT * FROM
        (
        WITH cte AS (SELECT * FROM foo)
        SELECT * FROM (SELECT * FROM cte UNION ALL SELECT * FROM cte)
        AS X
        JOIN bar ON b = c
        ) AS XY
        JOIN jazz on c = e AND b = f;
$query$ AS qry \gset
-- We are very particular about this plan shape and data distribution with ORCA:
-- 1. `jazz` has to be the inner table of the outer HASH JOIN, so that on a
-- segment which has zero tuples in `jazz`, the Sequence node that contains the
-- Shared Scan will be squelched on that segment. If `jazz` is not on the inner
-- side, the above mentioned "hang" scenario will not be covered.
-- 2. The Shared Scan producer has to be on a different slice from consumers,
-- and some tuples coming out of the Share Scan producer on one segments are
-- redistributed to a different segment over Motion. If not, the above mentioned
-- "wrong result" scenario will not be covered.
EXPLAIN (COSTS OFF)
:qry ;
                               QUERY PLAN                               
------------------------------------------------------------------------
 Gather Motion 3:1  (slice3; segments: 3)
   ->  Hash Join
         Hash Cond: ((bar.c = jazz.e) AND (foo.b = jazz.f))
         ->  Hash Join
               Hash Cond: (bar.c = foo.b)
               ->  Seq Scan on bar
               ->  Hash
                     ->  Redistribute Motion 3:3  (slice1; segments: 3)
                           Hash Key: foo.b
                           ->  Append
                                 ->  Seq Scan on foo
                                 ->  Seq Scan on foo foo_1
         ->  Hash
               ->  Redistribute Motion 3:3  (slice2; segments: 3)
                     Hash Key: jazz.f
                     ->  Seq Scan on jazz
 Optimizer: Postgres query optimizer
(17 rows)

SET statement_timeout = '15s';
:qry ;
 a | b | c | d | e | f 
---+---+---+---+---+---
 1 | 2 | 2 | 2 | 2 | 2
 1 | 2 | 2 | 2 | 2 | 2
(2 rows)

RESET statement_timeout;
SELECT COUNT(*)
FROM (SELECT *,
        (
        WITH cte AS (SELECT * FROM jazz WHERE jazz.e = bar.c)
        SELECT 1 FROM cte c1, cte c2
        )
      FROM bar) as s;
 count 
-------
   100
(1 row)

CREATE TABLE t1 (a int, b int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE TABLE t2 (a int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
-- ORCA plan contains a Shared Scan producer with a unsorted Motion below it
EXPLAIN (COSTS OFF)
WITH cte AS (SELECT * FROM t1 WHERE random() < 0.1 LIMIT 10) SELECT a, 1, 1 FROM cte JOIN t2 USING (a);
                                          QUERY PLAN                                          
----------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice3; segments: 3)
   ->  Hash Join
         Hash Cond: (t2.a = cte.a)
         ->  Seq Scan on t2
         ->  Hash
               ->  Redistribute Motion 1:3  (slice2; segments: 1)
                     Hash Key: cte.a
                     ->  Subquery Scan on cte
                           ->  Limit
                                 ->  Gather Motion 3:1  (slice1; segments: 3)
                                       ->  Limit
                                             ->  Seq Scan on t1
                                                   Filter: (random() < 0.1::double precision)
 Optimizer: Postgres query optimizer
(14 rows)

-- This functions returns one more column than expected.
CREATE OR REPLACE FUNCTION col_mismatch_func1() RETURNS TABLE (field1 int, field2 int)
LANGUAGE 'plpgsql' VOLATILE STRICT AS
$$
DECLARE
   v_qry text;
BEGIN
   v_qry := 'WITH cte AS (SELECT * FROM t1 WHERE random() < 0.1 LIMIT 10) SELECT a, 1 , 1 FROM cte JOIN t2 USING (a)';
  RETURN QUERY EXECUTE v_qry;
END
$$;
-- This should only ERROR and should not SIGSEGV
SELECT col_mismatch_func1();
ERROR:  structure of query does not match function result type
DETAIL:  Number of returned columns (3) does not match expected column count (2).
CONTEXT:  PL/pgSQL function col_mismatch_func1() line 6 at RETURN QUERY
-- ORCA plan contains a Shared Scan producer with a sorted Motion below it
EXPLAIN (COSTS OFF)
WITH cte AS (SELECT * FROM t1 WHERE random() < 0.1 ORDER BY b LIMIT 10) SELECT a, 1, 1 FROM cte JOIN t2 USING (a);
                                             QUERY PLAN                                             
----------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice3; segments: 3)
   ->  Hash Join
         Hash Cond: (t2.a = cte.a)
         ->  Seq Scan on t2
         ->  Hash
               ->  Redistribute Motion 1:3  (slice2; segments: 1)
                     Hash Key: cte.a
                     ->  Subquery Scan on cte
                           ->  Limit
                                 ->  Gather Motion 3:1  (slice1; segments: 3)
                                       Merge Key: t1.b
                                       ->  Limit
                                             ->  Sort
                                                   Sort Key: t1.b
                                                   ->  Seq Scan on t1
                                                         Filter: (random() < 0.1::double precision)
 Optimizer: Postgres query optimizer
(17 rows)

--- This functions returns one more column than expected.
CREATE OR REPLACE FUNCTION col_mismatch_func2() RETURNS TABLE (field1 int, field2 int)
    LANGUAGE 'plpgsql' VOLATILE STRICT AS
$$
DECLARE
    v_qry text;
BEGIN
    v_qry := 'WITH cte AS (SELECT * FROM t1 WHERE random() < 0.1 ORDER BY b LIMIT 10) SELECT a, 1 , 1 FROM cte JOIN t2 USING (a)';
    RETURN QUERY EXECUTE v_qry;
END
$$;
-- This should only ERROR and should not SIGSEGV
SELECT col_mismatch_func2();
ERROR:  structure of query does not match function result type
DETAIL:  Number of returned columns (3) does not match expected column count (2).
CONTEXT:  PL/pgSQL function col_mismatch_func2() line 6 at RETURN QUERY
-- https://github.com/greenplum-db/gpdb/issues/12701
-- Disable cte sharing in subquery
drop table if exists pk_list;
NOTICE:  table "pk_list" does not exist, skipping
create table pk_list (id int, schema_name varchar, table_name varchar) distributed by (id);
drop table if exists calender;
NOTICE:  table "calender" does not exist, skipping
create table calender (id int, data_hour timestamp) distributed by (id);
explain (costs off)
with
	tbls as (select distinct schema_name, table_name as table_nm from pk_list),
	tbls_daily_report_23 as (select unnest(string_to_array('mart_cm.card' ,',')) as table_nm_23),
	tbls_w_onl_actl_data as (select unnest(string_to_array('mart_cm.cont_resp,mart_cm.card', ',')) as table_nm_onl_act)
select  data_hour, stat.schema_name as schema_nm, dt.table_nm
from (
	select * from calender c
	cross join tbls
) dt
inner join (
	select tbls.schema_name, tbls.table_nm as table_name
	from tbls tbls
) stat on dt.table_nm = stat.table_name
where
	(data_hour = date_trunc('day',data_hour) and stat.schema_name || '.' ||stat.table_name not in (select table_nm_23 from tbls_daily_report_23))
	and (stat.schema_name || '.' ||stat.table_name not in (select table_nm_onl_act from tbls_w_onl_actl_data))
	or (stat.schema_name || '.' ||stat.table_name in (select table_nm_onl_act from tbls_w_onl_actl_data));
                                                                           QUERY PLAN                                                                            
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice11; segments: 3)
   ->  Hash Join
         Hash Cond: ((dt.table_nm)::text = (stat.table_name)::text)
         Join Filter: (((dt.data_hour = date_trunc('day'::text, dt.data_hour)) AND (NOT (hashed SubPlan 1)) AND (NOT (hashed SubPlan 2))) OR (hashed SubPlan 3))
         ->  Subquery Scan on dt
               ->  Nested Loop
                     ->  Seq Scan on calender c
                     ->  Materialize
                           ->  Broadcast Motion 3:3  (slice2; segments: 3)
                                 ->  HashAggregate
                                       Group Key: pk_list.schema_name, pk_list.table_name
                                       ->  Redistribute Motion 3:3  (slice1; segments: 3)
                                             Hash Key: pk_list.schema_name, pk_list.table_name
                                             ->  HashAggregate
                                                   Group Key: pk_list.schema_name, pk_list.table_name
                                                   ->  Seq Scan on pk_list
         ->  Hash
               ->  Broadcast Motion 3:3  (slice7; segments: 3)
                     ->  Subquery Scan on stat
                           Filter: (((NOT (hashed SubPlan 1)) AND (NOT (hashed SubPlan 2))) OR (hashed SubPlan 3))
                           ->  HashAggregate
                                 Group Key: pk_list_1.schema_name, pk_list_1.table_name
                                 ->  Redistribute Motion 3:3  (slice6; segments: 3)
                                       Hash Key: pk_list_1.schema_name, pk_list_1.table_name
                                       ->  HashAggregate
                                             Group Key: pk_list_1.schema_name, pk_list_1.table_name
                                             ->  Seq Scan on pk_list pk_list_1
                           SubPlan 1  (slice7; segments: 3)
                             ->  Materialize
                                   ->  Broadcast Motion 1:3  (slice3; segments: 1)
                                         ->  Result
                           SubPlan 2  (slice7; segments: 3)
                             ->  Materialize
                                   ->  Broadcast Motion 1:3  (slice4; segments: 1)
                                         ->  Result
                           SubPlan 3  (slice7; segments: 3)
                             ->  Materialize
                                   ->  Broadcast Motion 1:3  (slice5; segments: 1)
                                         ->  Result
         SubPlan 1  (slice11; segments: 3)
           ->  Materialize
                 ->  Broadcast Motion 1:3  (slice8; segments: 1)
                       ->  Result
         SubPlan 2  (slice11; segments: 3)
           ->  Materialize
                 ->  Broadcast Motion 1:3  (slice9; segments: 1)
                       ->  Result
         SubPlan 3  (slice11; segments: 3)
           ->  Materialize
                 ->  Broadcast Motion 1:3  (slice10; segments: 1)
                       ->  Result
 Optimizer: Postgres query optimizer
(52 rows)

-- Test the scenario which already opened many fds
-- start_ignore
RESET search_path;
-- end_ignore
\! mkdir -p /tmp/_gpdb_fault_inject_tmp_dir/
select gp_inject_fault('inject_many_fds_for_shareinputscan', 'skip', dbid) from gp_segment_configuration where role = 'p' and content = 0;
 gp_inject_fault 
-----------------
 Success:
(1 row)

-- borrow the test query in gp_aggregates
select case when ten < 5 then ten else ten * 2 end, count(distinct two), count(distinct four) from tenk1 group by 1;
 case | count | count 
------+-------+-------
    0 |     1 |     2
    1 |     1 |     2
    2 |     1 |     2
    3 |     1 |     2
    4 |     1 |     2
   10 |     1 |     2
   12 |     1 |     2
   14 |     1 |     2
   16 |     1 |     2
   18 |     1 |     2
(10 rows)

select gp_inject_fault('inject_many_fds_for_shareinputscan', 'reset', dbid) from gp_segment_configuration where role = 'p' and content = 0;
 gp_inject_fault 
-----------------
 Success:
(1 row)

\! rm -rf /tmp/_gpdb_fault_inject_tmp_dir/
-- To be able to pass this test, Shared Scan's Material node should be marked
-- as cross-slice. Previously, we processed all subplans separately from main
-- plan, which caused Material node not marked as cross-slice (upper main plan's
-- slice1 motion was ignored in processing).
-- No error like "ERROR:  cannot execute inactive Motion (nodeMotion.c:264)"
-- should be shown.
create table t1 (a int, b int, c int) distributed by (a);
explain (costs off)
with cte1 as (
  select max(c) as c from t1
), 
cte2 as (
  select d as c
  from generate_series(
    (select c from cte1) - 4,
    (select c from cte1), 1) d
)
select l.c from cte2 l
left join t1 u on l.c = u.c;
                             QUERY PLAN                             
--------------------------------------------------------------------
 Hash Right Join
   Hash Cond: (u.c = d.d)
   ->  Gather Motion 3:1  (slice1; segments: 3)
         ->  Seq Scan on t1 u
   ->  Hash
         ->  Function Scan on generate_series d
               InitPlan 1 (returns $0)  (slice4)
                 ->  Aggregate
                       ->  Gather Motion 3:1  (slice2; segments: 3)
                             ->  Aggregate
                                   ->  Seq Scan on t1
               InitPlan 2 (returns $1)  (slice5)
                 ->  Aggregate
                       ->  Gather Motion 3:1  (slice3; segments: 3)
                             ->  Aggregate
                                   ->  Seq Scan on t1 t1_1
 Optimizer: Postgres query optimizer
(17 rows)

with cte1 as (
  select max(c) as c from t1
), 
cte2 as (
  select d as c
  from generate_series(
    (select c from cte1) - 4,
    (select c from cte1), 1) d
)
select l.c from cte2 l
left join t1 u on l.c = u.c;
 c 
---
(0 rows)

-- This case shows flacky count(*) result on pre-patched version. To make it
-- stable wrong we use new fault point.
-- Shared Scan consumer from slice1 not marked as cross-slice and not
-- initialized tuple store. We got '1' as the result of the query - only
-- Shared Scan from slice2 (the part below UNION) executed correctly.
-- From now, cross-slice interaction detection fixed for subplans and we have
-- stable '100' as a result.
create table t2(i int, j int) distributed by (i);
insert into t2 select i, i * 10 from generate_series(1, 10) i;
select gp_inject_fault('material_pre_tuplestore_flush', 'reset', dbid)
from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:
(1 row)

select gp_inject_fault('material_pre_tuplestore_flush',
       'sleep', '', '', '', 1, 1, 5, dbid)
from gp_segment_configuration where role = 'p' and content = -1;       
 gp_inject_fault 
-----------------
 Success:
(1 row)

set optimizer_parallel_union = on;
explain (costs off)
with cte1 as (
  select max(j) as max_j from t2
)
select count(*) c
from (
  select s * 10 s
  from generate_series(1, (select max_j from cte1)) s
  union
  select max_j from cte1
) t;
                                QUERY PLAN                                
--------------------------------------------------------------------------
 Aggregate
   ->  HashAggregate
         Group Key: ((s.s * 10))
         ->  Append
               ->  Function Scan on generate_series s
                     InitPlan 1 (returns $0)  (slice3)
                       ->  Aggregate
                             ->  Gather Motion 3:1  (slice1; segments: 3)
                                   ->  Aggregate
                                         ->  Seq Scan on t2 t2_1
               ->  Aggregate
                     ->  Gather Motion 3:1  (slice2; segments: 3)
                           ->  Aggregate
                                 ->  Seq Scan on t2
 Optimizer: Postgres query optimizer
(15 rows)

with cte1 as (
  select max(j) as max_j from t2
)
select count(*) c
from (
  select s * 10 s
  from generate_series(1, (select max_j from cte1)) s
  union
  select max_j from cte1
) t;
  c  
-----
 100
(1 row)

reset optimizer_parallel_union;
select gp_inject_fault_infinite('material_pre_tuplestore_flush', 'reset', dbid)
from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault_infinite 
--------------------------
 Success:
(1 row)

